version: '3.8'

services:
  # --- SERVICE 1: YOUR SOC SYSTEM (APP + MCP) ---
  soc-assistant:
    build: .                          # Build using the Dockerfile in current folder
    container_name: wazuh-mcp-agent
    ports:
      - "8501:8501"                   # Expose Streamlit port on your host machine
    volumes:
      - .:/app                        # Live sync: edits on Windows appear in Docker
    environment:
      - OLLAMA_HOST=http://ollama:11434 # Tell your code where Ollama is located
    depends_on:
      - ollama                        # Start only after Ollama is ready
    restart: unless-stopped           # Auto-restart on failure

  # --- SERVICE 2: OLLAMA (AI ENGINE) ---
  ollama:
    image: ollama/ollama:latest       # Official Ollama image
    container_name: ollama-service
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama     # Persist models (Llama 3.2) to avoid re-downloading
    restart: unless-stopped
    # Note: On first run, you need to 'docker exec' to pull the model
    # Example: docker exec -it ollama-service ollama pull llama3.2

  # --- SERVICE 3: METASPLOITABLE (VULNERABLE TARGET) ---
  # Simulates a vulnerable Linux server for infrastructure attack scenarios
  metasploitable:
    image: tleemcjr/metasploitable2   # Metasploitable2 Docker image
    container_name: metasploitable-target
    # --- ADICIONA ESTAS DUAS LINHAS ---
    stdin_open: true # same as -i
    tty: true        # same as -t
    # ----------------------------------
    ports:
      - "2222:22"                     # SSH (mapped to 2222 to avoid host conflict)
      - "21:21"                       # FTP
      - "23:23"                       # Telnet
      - "8080:80"                     # HTTP
    restart: unless-stopped
    # WARNING: This is an intentionally vulnerable target
    # NEVER expose these ports to the internet - lab use only

# Volume to persist Ollama models
volumes:
  ollama_data: