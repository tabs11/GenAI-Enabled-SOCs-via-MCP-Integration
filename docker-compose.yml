version: '3.8'

services:
  # --- SERVICE 1: YOUR SOC SYSTEM (APP + MCP) ---
  soc-assistant:
    build: .                          # Build using the Dockerfile in current folder
    container_name: wazuh-mcp-agent
    ports:
      - "8501:8501"                   # Expose Streamlit port on your host machine
    volumes:
      - .:/app                        # Live sync: edits on Windows appear in Docker
    environment:
      - OLLAMA_HOST=http://ollama:11434 # Tell your code where Ollama is located
    depends_on:
      - ollama                        # Start only after Ollama is ready
    restart: unless-stopped           # Auto-restart on failure

  # --- SERVICE 2: OLLAMA (AI ENGINE) ---
  ollama:
    image: ollama/ollama:latest       # Official Ollama image
    container_name: ollama-service
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama     # Persist models (Llama 3.2) to avoid re-downloading
    restart: unless-stopped
    # Note: On first run, you need to 'docker exec' to pull the model
    # Example: docker exec -it ollama-service ollama pull llama3.2

  # --- SERVICE 3: VULNERABLE MACHINE (THE TARGET) ---
  # Optional: Simulates the target of attacks for demonstration
  victim-machine:
    image: vulnerables/web-dvwa       # Web app with known security vulnerabilities
    container_name: vulnerable-target
    ports:
      - "8080:80"                     # Access it in browser at http://localhost:8080
    restart: unless-stopped

# Volume to persist Ollama models
volumes:
  ollama_data: